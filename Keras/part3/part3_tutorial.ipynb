{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part3-tutorial.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxMTbLIW5rl",
        "colab_type": "text"
      },
      "source": [
        "# Learning to Deep Learn using Python, Keras, TensorFlow and a GPU\n",
        "\n",
        "_[Jonathon Hare, 21st Jan 2018](https://github.com/jonhare/DISCnetMachineLearningCourse)_\n",
        "\n",
        "## Change History\n",
        "\n",
        "- 20180121: Initial version\n",
        "- 20180416: Update for DISCnet\n",
        "- 20190408: Update for DISCnet/2 + Colab\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this final practical session we'll use Keras to model and analyse sequence data using recurrent neural networks made from computational blocks called a \"Long Short Term Memory\", or LSTM. In the first part of the tutorial we'll explore how we can predict language - given a starting character, can we predict what will come next? We'll start by implementing a simple \"1st-order Markov Chain\" to learn the transisition probabilities between characters, and we'll then compare this to a model that can learn longer-term dependencies using a recurrent neural network.\n",
        "\n",
        "The second part will look at sequence classification. Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence. What makes this problem difficult is that the sequences can vary in length, be comprised of a very large vocabulary of input symbols and may require the model to learn the long-term context or dependencies between symbols in the input sequence. We've already explored how we can overcome this problem using Bag of Word approaches, but we've also seen that BoWs have limitations because they ignore word order. N-grams were suggested as an alternative, but they have their own problems with feature explosion. In this exercise, you will discover how you can overcome these problems by developing LSTM recurrent neural network models for sequence classification problems.\n",
        "\n",
        "Through this part of the tutorial you'll learn how to:\n",
        "\n",
        "* How to learn a language model using a recurrent network & to sample the model to generate new language.\n",
        "* How to use callbacks during training to monitor progress.\n",
        "* How to develop an LSTM model for a sequence classification problem.\n",
        "\n",
        "## Acknowledgements\n",
        "The LSTM-based Nietzsche generator described in the first part of the tutorial comes from the Keras examples. The second part of this tutorial is largely based on the first section of Jason Brownlee's [\"Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras\"](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) tutorial. \n",
        "\n",
        "## Prerequisites\n",
        "To use this tutorial you'll use the Python 3 language with the `keras` deep learning library and the `tensorflow` backend. We'll also use the `scikit-learn` and `numpy` packages. For this lab we'll use a Jupyter notebook running in the cloud on [Google Colab](https://colab.research.google.com). Colab gives us free access to a virtual machine with GPU acceleration and all the prerequisite libraries pre-installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np1EWZLPW5rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a8390baa-bd21-4d61-c717-d81780d24f59"
      },
      "source": [
        "__Note:__ in Jupyter Notebooks, commands with an exclaimation mark (!) in front of them are shell commands, and will run just as if typed in a terminal (without the exclaimation mark)."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6c94ed5e67cf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    __Note:__ in Jupyter Notebooks, commands with an exclaimation mark (!) in front of them are shell commands, and will run just as if typed in a terminal (without the exclaimation mark).\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZCwVr2jW5rr",
        "colab_type": "text"
      },
      "source": [
        "If running locally you'll need access to a computer with the following installed:\n",
        "\n",
        "- `Python` (> 3.6)\n",
        "- `keras` (>= 2.0.0)\n",
        "- `tensorflow` (>= 1.0.0)\n",
        "- `NumPy` (>= 1.12.1)\n",
        "- `SciPy` (>= 0.19.1)\n",
        "- `scikit-learn` (>= 0.19.1)\n",
        "\n",
        "If you've installed the base Anaconda python distribution, then running `conda install keras` will install both keras and tensorflow. You can make a start on this tutorial using you own machines, however you'll find that the code runs rather slowly. To run at more sensible speeds you need access to a machine with a powerful GPU (or GPUs).\n",
        "\n",
        "## Modelling sequences\n",
        "\n",
        "### Markov chains\n",
        "\n",
        "We'll start our exploration of modelling sequences and building generative models using a 1st order Markov chain. The Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. In our case we're going to learn a model over a set of characters from an English language text. The events, or states, in our model are the set of possible characters, and we'll learn the probability of moving from one character to the next.\n",
        "\n",
        "Let's start by loading the data from the web:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amdYzOsaW5rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5797b916-de84-4dbe-fbdf-f85c57acc7cc"
      },
      "source": [
        "from keras.utils.data_utils import get_file\n",
        "!pip install numpy==1.16.1\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# Read the data\n",
        "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = io.open(path, encoding='utf-8').read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7wJ9-PMW5rw",
        "colab_type": "text"
      },
      "source": [
        "We now need to iterate over the characters in the text and count the times each transition happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aQAWOGgW5rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transition_counts = dict()\n",
        "for i in range(0,len(text)-1):\n",
        "\tcurrc = text[i]\n",
        "\tnextc = text[i+1]\n",
        "\tif currc not in transition_counts:\n",
        "\t\ttransition_counts[currc] = dict()\n",
        "\tif nextc not in transition_counts[currc]:\n",
        "\t\ttransition_counts[currc][nextc] = 0\n",
        "\ttransition_counts[currc][nextc] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDg0KfOFW5rz",
        "colab_type": "text"
      },
      "source": [
        "The `transition_counts` dictionary maps the current character to the next character, and this is then mapped to a count. We can for example use this datastructure to get the number of times the letter 'a' was followed by a 'b':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb1EfzcZW5r0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6bd7478-b8ef-4c17-f8c6-2fa97083e1ad"
      },
      "source": [
        "print(\"Number of transitions from 'a' to 'b': \" + str(transition_counts['a']['b']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of transitions from 'a' to 'b': 813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFeWCO4_W5r3",
        "colab_type": "text"
      },
      "source": [
        "which, if we run the code at this point should print:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlJ96kJeW5r3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "38370591-f9b3-4680-d107-8b7e23e9d18e"
      },
      "source": [
        "Number of transitions from 'a' to 'b': 813"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-37dc89a9edb4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Number of transitions from 'a' to 'b': 813\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pyEEOZbW5r6",
        "colab_type": "text"
      },
      "source": [
        "Finally, to complete the model we need to normalise the counts for each initial character into a probability distribution over the possible next character. We'll slightly modify the form we're storing these and maintain a pair of array objects for each initial character: the first holding the set of possible characters, and the second holding the corresponding probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50SbMMySW5r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transition_probabilities = dict()\n",
        "for currentc, next_counts in transition_counts.items():\n",
        "\tvalues = []\n",
        "\tprobabilities = []\n",
        "\tsumall = 0\n",
        "\tfor nextc, count in next_counts.items():\n",
        "\t\tvalues.append(nextc)\n",
        "\t\tprobabilities.append(count)\n",
        "\t\tsumall += count\n",
        "\tfor i in range(0, len(probabilities)):\n",
        "\t\tprobabilities[i] /= float(sumall)\n",
        "\ttransition_probabilities[currentc] = (values, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1sErE2TW5r9",
        "colab_type": "text"
      },
      "source": [
        "At this point, we could print out the probability distribution for a given initial character state. For example, to print the distribution for 'a':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiKpjHHWW5r-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "9741d259-83b4-4c77-d360-5fc153488698"
      },
      "source": [
        "for a,b in zip(transition_probabilities['a'][0], transition_probabilities['a'][1]):\n",
        "\tprint(a,b)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c 0.03685183172083922\n",
            "t 0.14721708881400153\n",
            "  0.05296771388194369\n",
            "n 0.2322806826829003\n",
            "l 0.11552886183280792\n",
            "r 0.08794434177628004\n",
            "s 0.0968583541689314\n",
            "v 0.0192412218719426\n",
            "i 0.03402543754755952\n",
            "d 0.026986628981411024\n",
            "g 0.017202956843135123\n",
            "y 0.02505707142080661\n",
            "k 0.012827481247961734\n",
            "b 0.02209479291227307\n",
            "p 0.020545711490379388\n",
            "m 0.02030111968692249\n",
            "u 0.011414284161321883\n",
            "f 0.004429829329274921\n",
            "w 0.004837482335036417\n",
            ", 0.0010870746820306554\n",
            "\n",
            " 0.005353842809000978\n",
            "z 0.0006522448092183933\n",
            "x 0.0007609522774214588\n",
            "o 0.0005435373410153277\n",
            ". 0.000489183606913795\n",
            "- 0.0004348298728122622\n",
            "' 5.4353734101532776e-05\n",
            "j 0.0004348298728122622\n",
            "h 0.00035329927165996303\n",
            "e 0.0007337754103706925\n",
            ": 5.4353734101532776e-05\n",
            "a 5.4353734101532776e-05\n",
            ") 0.00010870746820306555\n",
            "! 2.7176867050766388e-05\n",
            "; 2.7176867050766388e-05\n",
            "\" 8.153060115229916e-05\n",
            "q 2.7176867050766388e-05\n",
            "_ 8.153060115229916e-05\n",
            "[ 2.7176867050766388e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa_8vdo8W5sC",
        "colab_type": "text"
      },
      "source": [
        "The output should look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY0kVTnfW5sC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "218aeeec-835c-4800-d62f-c847e20d3790"
      },
      "source": [
        "c 0.03685183172083922\n",
        "t 0.14721708881400153\n",
        "  0.05296771388194369\n",
        "n 0.2322806826829003\n",
        "l 0.11552886183280792\n",
        "r 0.08794434177628004\n",
        "s 0.0968583541689314\n",
        "v 0.0192412218719426\n",
        "i 0.03402543754755952\n",
        "d 0.026986628981411024\n",
        "g 0.017202956843135123\n",
        "y 0.02505707142080661\n",
        "k 0.012827481247961734\n",
        "b 0.02209479291227307\n",
        "p 0.020545711490379388\n",
        "m 0.02030111968692249\n",
        "u 0.011414284161321883\n",
        "f 0.004429829329274921\n",
        "w 0.004837482335036417\n",
        ", 0.0010870746820306554\n",
        "\n",
        " 0.005353842809000978\n",
        "z 0.0006522448092183933\n",
        "x 0.0007609522774214588\n",
        "o 0.0005435373410153277\n",
        ". 0.000489183606913795\n",
        "- 0.0004348298728122622\n",
        "' 5.4353734101532776e-05\n",
        "j 0.0004348298728122622\n",
        "h 0.00035329927165996303\n",
        "e 0.0007337754103706925\n",
        ": 5.4353734101532776e-05\n",
        "a 5.4353734101532776e-05\n",
        ") 0.00010870746820306555\n",
        "! 2.7176867050766388e-05\n",
        "; 2.7176867050766388e-05\n",
        "\" 8.153060115229916e-05\n",
        "q 2.7176867050766388e-05\n",
        "_ 8.153060115229916e-05\n",
        "[ 2.7176867050766388e-05"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-0471114ec468>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    c 0.03685183172083922\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcvinKqBW5sF",
        "colab_type": "text"
      },
      "source": [
        "It looks like the most probable letter to follow an 'a' is 'n'. \n",
        "\n",
        "We mentioned earlier that the Markov model is generative. This means that we can draw samples from the distributions and iteratively move between states. This can generate text (starting here from a 't'): \n",
        "\n",
        " ```python\n",
        "# sample\n",
        "current = 't'\n",
        "for i in range(0, 1000):\n",
        "\tprint(current, end='')\n",
        "\tvalues, probabilities = transition_probabilities[current]\n",
        "\tcurrent = np.random.choice(values, p=probabilities)\n",
        " ```\n",
        "\n",
        "Running this should generate some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hF2tHltW5sG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "c2027db3-13dd-41a3-b098-d4687f774836"
      },
      "source": [
        "thy\n",
        "chonevese juanongexpstiserorrean s antond f atomoulthed ng ives re whe ofig stouc thevey f esio daron ug nrulint se ag end tieve tepe ve ugis wintas od crtire, in miowaver, ting trypal utid cut  the s t athet, ckecthictr rak orlvencowhe\n",
        "s par.\n",
        "mosomunurrtontimas ply by\n",
        "apedl ws n onchanese intomes whution, mergerinse pr f-nces: irget ad\n",
        "msieedis, wnts oflerer hevecy o ch\n",
        "(ausa ilf psherit ther jend; nd f soto vesay heshtugis on gutf\n",
        "\n",
        "io seted, t ise woly obase anconarored a akighemoind theentheh sthystimathevabysewin thesthe\n",
        "t o owere\n",
        "fofifithecofffe bof al\n",
        "\n",
        "chouereste geapunin, thoneyeresthade iotsemassct, icthino tuphelite-and enoninse de\n",
        "rerche mabo, t h id tsteavinablyphd ong.-wang cy on andior\n",
        "of me haben).\n",
        "anatos, thins\n",
        "on pouce thimendind, pe sthe quche wo s lere tog akalin engs].\n",
        "n sor cheokisunly: osse dla outhes, be t triksthithe\n",
        "isttatonnnf s kinsule of e\n",
        "\n",
        "mesif st nar to\n",
        "erof himasthiouspud simofe, harnt dsous. ad ikimofrt lienderer, k oe toneser, dg n be tbe ced tot\n",
        "pl"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-b9a994b8518d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    chonevese juanongexpstiserorrean s antond f atomoulthed ng ives re whe ofig stouc thevey f esio daron ug nrulint se ag end tieve tepe ve ugis wintas od crtire, in miowaver, ting trypal utid cut  the s t athet, ckecthictr rak orlvencowhe\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WEca24tW5sI",
        "colab_type": "text"
      },
      "source": [
        "This is clearly not English, but it's obvious that some of the common structures in the English language have been captured.\n",
        "\n",
        "> __Exercise:__ Rather than building a model based on individual characters, can you modify the model to work on words instead?\n",
        "\n",
        "### RNN-based sequence modelling\n",
        "\n",
        "It is possible to build higher-order Markov models that capture longer-term dependencies in the text and have higher accuracy, however this does tend to become computationally infeasible very quickly. Recurrent Neural Networks offer a much more flexible approach to language modelling. \n",
        "\n",
        "To get started, as with out Markov model we need to start by loading the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-IJSlYvW5sJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d4243c8-2051-4a42-f717-25cf38977621"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# Read the data\n",
        "path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = io.open(path, encoding='utf-8').read().lower()\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqWxx2aAW5sL",
        "colab_type": "text"
      },
      "source": [
        "We'll need to create mappings of characters to numeric indices (and vice-versa) in order to perform the one-hot encoding of each character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooY7PWkqW5sM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "163061f6-66ce-438e-ad96-fd90e53299cb"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5iQ35e-W5sO",
        "colab_type": "text"
      },
      "source": [
        "We now need to prepare the data into shorter subsequences that we can use to train the model. We'll make these redundant by overlapping them. Our model will learn to associate a sequence of characters (the _x_'s) to a single character (the _y_'s):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQIdUZURW5sP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "678fd89f-85a9-4a48-e11c-c68d6b3b3c6b"
      },
      "source": [
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 200285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-uGS7pTW5sR",
        "colab_type": "text"
      },
      "source": [
        "The final step is to apply our character mapping to each subsequence and one-hot encode the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jwNZyd9W5sS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bfea118-2f81-492f-8197-66747cd4ce22"
      },
      "source": [
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcOu-8DTW5sU",
        "colab_type": "text"
      },
      "source": [
        "We can now define the model. We'll use a simple LSTM followed by a dense layer with a softmax to predict probabilities against each character in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejAAKvnFW5sV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d3ddfea6-0600-46b1-874b-f360ec921ecb"
      },
      "source": [
        "# build the model: a single LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glKDFa74W5sY",
        "colab_type": "text"
      },
      "source": [
        "We could train our model at this point, but it would be nice to be able to sample it during training so we can see how its learning. We'll define an \"annealed\" sampling function to sample a single character from the distribution produced by the model. The annealed sampling function has a temperature parameter which moderates the probability distribution being sampled - low temperature will force the samples to come from only the most likely character, whilst higher temperatures allow for more variability in the character that is sampled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPlo8CXsW5sY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb3bko0lW5sd",
        "colab_type": "text"
      },
      "source": [
        "Keras lets us define 'hooks' or callbacks which can be triggered during training (for example at the end of each epoch). Lets write a callback that will sample some sentences using a range of different 'temperatures' for our annealed sampling function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LvOO8oqW5se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uCC446W5sh",
        "colab_type": "text"
      },
      "source": [
        "Finally we can train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUNN9SZ4W5si",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "09a9d624-a882-4c07-b27e-4fb974186ad6"
      },
      "source": [
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=1,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "200285/200285 [==============================] - 93s 464us/step - loss: 2.0012\n",
            "\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ost incredible, is rendered less\n",
            "atrocio\"\n",
            "ost incredible, is rendered less\n",
            "atrocions of the propose the perpose the proford of the sense of the profound the sense the hand of the morals of the privents of the preases the sense of the profound and what is the prease of the proford the condical the perhaps to the bechope of the profound the proped the sense of the priven and the self the seeple the religion of the profound and the perpose the profound the sense of the preared the\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ost incredible, is rendered less\n",
            "atrocio\"\n",
            "ost incredible, is rendered less\n",
            "atrocious of the \"confucal preat the beartous and man \"what the regard, know\n",
            "detasing them is the seever and the perpose the preatunted and be in the preases able and love with morals, in the conseduced to hand but fored one the staid to happenated of unier a stand, and the reart and influence of the deapotion of the german in the conditual which expected man forme acciman and incestanding and accoment b\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"ost incredible, is rendered less\n",
            "atrocio\"\n",
            "ost incredible, is rendered less\n",
            "atrocion of bearnes, sablether sirpluig, difficice is. are beloseng wise, he in us\n",
            "in may condic um\n",
            "cluligenc: it morand\n",
            "intolment,\n",
            "the ccienticis of his tays of colfece. the idself, whenesrial philosophy, been an sobself,\n",
            "the lation, desiging, and wither the theinging\n",
            "tenects onen victed to the tskens of\n",
            "infects of the absimpre with to be be aureeness wanding to the where seefficainirity with their low \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"ost incredible, is rendered less\n",
            "atrocio\"\n",
            "ost incredible, is rendered less\n",
            "atrociov tot artuder: mey \"permagance diffekent cultutal prepedyib\n",
            "levely , encouge whon he stidek out brapfous shales,, it warp hicd the cnatenest estife\". accomes of great believedness and arcadinacce beanfers, but feerestical upon catee mownistuble and\n",
            "ispyutiou (andta, es a realsh lams of ceurene, of\n",
            "thing, baugn, and hame cruedtion sciendes and over opealse\n",
            "ater, he (anour--pleas, ismen\n",
            "on created\n",
            " \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff9e55c36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEpAvEP4W5sk",
        "colab_type": "text"
      },
      "source": [
        "Running the code will produce output like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViMV23oW5sl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c263b83c-7f48-422f-863e-a7b6c02dce0d"
      },
      "source": [
        "Using TensorFlow backend.\n",
        "corpus length: 600893\n",
        "total chars: 57\n",
        "nb sequences: 200285\n",
        "Vectorization...\n",
        "Build model...\n",
        "Epoch 1/60\n",
        "name: GeForce GTX TITAN X\n",
        "major: 5 minor: 2 memoryClockRate (GHz) 1.076\n",
        "pciBusID 0000:02:00.0\n",
        "Total memory: 11.92GiB\n",
        "Free memory: 385.50MiB\n",
        "200192/200285 [============================>.] - ETA: 0s - loss: 1.9911  \n",
        "----- Generating text after Epoch: 0\n",
        "----- diversity: 0.2\n",
        "----- Generating with seed: \"olitude; in his strongest words, even in\"\n",
        "olitude; in his strongest words, even in the soched and the sount of the sore of the sight of the so the segratic upon the sounting and the self-even the sorting the so have sourther the something and the so the sounting the sortance and the so fact in the self-present and the entingly and the so findent that the\n",
        "something the something of the something the self-precenting one the sore of the sould of the self-enoughther of the self-con\n",
        "----- diversity: 0.5\n",
        "----- Generating with seed: \"olitude; in his strongest words, even in\"\n",
        "olitude; in his strongest words, even in the something in the endordly and stringled in the outher to has he becoungs and the sigin of the sore of a still the so without as the sich has he herther he however and the sensity and the weach of is the intinle of the prosention and oftance in their tay has atianly the fitht of sectly in any religion to hand can when when as to the outht with the ent and the sentume the strangatical to the so\n",
        "----- diversity: 1.0\n",
        "----- Generating with seed: \"olitude; in his strongest words, even in\"\n",
        "olitude; in his strongest words, even in leattured to gable or elrogacal to-stranline,\n",
        "by the prive by etrests\n",
        "exiltivesan. everywent liken\n",
        "heant weth the mouting\n",
        "which sinch, conceptionory to to to , the\n",
        "in aresus and beunimanly beturally suck\n",
        "more\n",
        "of imponsies freatack of which also\n",
        "the wetu canies, the ,\n",
        "when these noulble, they eastonce prictian\n",
        "still, a gart been\n",
        "our tombonge, cistentated in this sopess. the\n",
        "then dircapts.\n",
        "\n",
        " antace\n",
        "----- diversity: 1.2\n",
        "----- Generating with seed: \"olitude; in his strongest words, even in\"\n",
        "olitude; in his strongest words, even in these vonatingles histyeuse--thas esters\"-gyrty; he n reliend ischilal; \"that is in a how patking it our doschys, in stren irmig-to in\n",
        "espt?\n",
        "obe\"daday unevirti\n",
        "al simallicarns only excuanity. the it is however it.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " the creees of anti2liok, \n",
        "hist those for the ienested of\n",
        "the detiwly, a lingerse and womannie scsys;ite as reac, astroutuln?edyidity.\n",
        "cacturelyce\n",
        "fover myst.\n",
        "this owinglusic in rin\n",
        "\n",
        "...\n",
        "\n",
        "Epoch 10/60\n",
        "200192/200285 [============================>.] - ETA: 0s - loss: 1.3943  \n",
        "----- Generating text after Epoch: 9\n",
        "----- diversity: 0.2\n",
        "----- Generating with seed: \"music. but with regard to robert schuman\"\n",
        "music. but with regard to robert schuman to the seems to the sentiments of the same proper to the sentiments and among the same prospons of the same philosopher and the most say, which the first to the same freedom of the sentiments of the stands and a strength, and the man and the sentiment of the soul of the same time of the same philosophers and delight of the sentiments of the sentiments of the sentiment of the same man and strength\n",
        "----- diversity: 0.5\n",
        "----- Generating with seed: \"music. but with regard to robert schuman\"\n",
        "music. but with regard to robert schuman\n",
        "be species and and invented and men as the\n",
        "the prose of the early one should be a most has prouds to our coles and to the absolute it is all the most religion, of its experience, of the soul with the extended in the powerful, the order of a fruets of self-consequences of all the stain himself and preserved the extendance, and\n",
        "interpered to be be a period of false and whole the stable and conseque\n",
        "----- diversity: 1.0\n",
        "----- Generating with seed: \"music. but with regard to robert schuman\"\n",
        "music. but with regard to robert schumanly\n",
        "to pleasard for-ledist is to\n",
        "nobles therely\n",
        "gradually as remath a conditions,\" whether\n",
        "count is, as every pointed does not all as uniteing ent as there his empentaining from it, it was as heregation. leays the\n",
        "religious old heartstop ailly-wensable morald\n",
        "more pre\"ful man to be the christeal consequence, no powers of sortable would really isw. cimilical cost account.\n",
        "\n",
        "     religions how hhad. t\n",
        "----- diversity: 1.2\n",
        "----- Generating with seed: \"music. but with regard to robert schuman\"\n",
        "music. but with regard to robert schumanceophists,\" ones that for certaync-valk would, for eduming spirituals from loftiess, re-our higher experiences, hissically sutklio: if you\n",
        "jusk be a henceress givery, else is they serve apparered jane-man, just. man-freeds of !\n",
        "\n",
        "\n",
        "yo wotl\n",
        "plowe. every acted with the musimfured, must allopes, \n",
        "one.\n",
        "\n",
        "urorange sometempimal as treeess something and interf upon used for this\n",
        "preseme\n",
        "defouncious,\n",
        "conhol "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-6b8d1ba25710>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Using TensorFlow backend.\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ31DlppW5sn",
        "colab_type": "text"
      },
      "source": [
        "Looking at the results its possible to see the model works a bit like the Markov chain at the first epoch, but as the parameters become better tuned to the data it's clear that the LSTM has been able to model the structure of the language & is able to produce completely legible text.\n",
        "\n",
        "> __Exercise:__ Try adding another LSTM layer or two to the network. How does this affect performance?\n",
        "\n",
        "## Sequence Classification\n",
        "The problem that we will use to demonstrate sequence classification in this tutorial is the IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified.\n",
        "\n",
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment. The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
        "\n",
        "Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers.\n",
        "\n",
        "### Word Embedding\n",
        "\n",
        "We will map each movie review into a real vector domain using a popular technique when working with text called word embedding. Unlike one-hot encoding of words, a word embedding has a much lower dimensionality, and is designed to be able to capture synonomy. Word embedding is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. \n",
        "\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
        "\n",
        "We will map each word onto a 32 length real valued vector. We will also limit the total number of words that we are interested in modeling to the 5000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so we will constrain each review to be 500 words, truncating long reviews and pad the shorter reviews with zero values.\n",
        "\n",
        "Now that we have defined our problem and how the data will be prepared and modeled, we are ready to develop an LSTM model to classify the sentiment of movie reviews.\n",
        "\n",
        "### Simple LSTM for Sequence Classification\n",
        "\n",
        "We can quickly develop a small LSTM for the IMDB problem and achieve good accuracy.\n",
        "\n",
        "Letâ€™s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On3DOTvgW5sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPjD56jPW5ss",
        "colab_type": "text"
      },
      "source": [
        "We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-iaz1RdW5st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2DtpgD2W5sv",
        "colab_type": "text"
      },
      "source": [
        "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIqkUQxW5sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx2iYJIGW5sx",
        "colab_type": "text"
      },
      "source": [
        "We can now define, compile and fit our LSTM model.\n",
        "\n",
        "The first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
        "\n",
        "Because it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 2 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQhePkbjW5sy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "58e7243e-37ac-4f58-b4f0-5799879c1276"
      },
      "source": [
        "# create the model\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=64)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/1\n",
            "25000/25000 [==============================] - 384s 15ms/step - loss: 0.4613 - acc: 0.7756 - val_loss: 0.3410 - val_acc: 0.8595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff9abdf25f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbpLF0GbW5s1",
        "colab_type": "text"
      },
      "source": [
        "Once fit, we estimate the performance of the model on unseen reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF39rHoAW5s2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5bd0876-9b79-4dbf-9eec-2cf7373059b3"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkl-npUXW5s6",
        "colab_type": "text"
      },
      "source": [
        "Running this example produces the following output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6qPyTfTW5s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epoch 1/3\n",
        "16750/16750 [==============================] - 107s - loss: 0.5570 - acc: 0.7149\n",
        "Epoch 2/3\n",
        "16750/16750 [==============================] - 107s - loss: 0.3530 - acc: 0.8577\n",
        "Epoch 3/3\n",
        "16750/16750 [==============================] - 107s - loss: 0.2559 - acc: 0.9019\n",
        "Accuracy: 86.79%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzlqYQLnW5s8",
        "colab_type": "text"
      },
      "source": [
        "You can see that this simple LSTM with little tuning achieves near state-of-the-art results on the IMDB problem. Importantly, this is a template that you can use to apply LSTM networks to your own sequence classification problems.\n",
        "\n",
        "> __Exercise:__ What is the effect of changing the embedding length?"
      ]
    }
  ]
}
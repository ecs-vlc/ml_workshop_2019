{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this tutorial, learn Decision Tree Classification, attribute selection measures, and how to build and optimize a Decision Tree Classifier using Python Scikit-learn package. Based on the following article: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents a feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in a recursive manner, imaginatively called recursive partitioning. The learned tree can be visualised as a flowchart and is hopefully easy to interpret...\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "The basic idea behind any decision tree algorithm is as follows:\n",
    "\n",
    "1. Place the best attribute, selected according to an Attribute Selection Measure (ASM), of the dataset at the root of the tree.\n",
    "2. Split the training set into subsets. Subsets should be made in such a way that each subset contains data with the same value for an attribute.\n",
    "3. Repeat step 1 and step 2 on each subset until you find leaf nodes in all the branches of the tree.\n",
    "\n",
    "Various ASMs can be used, such as: Information gain, gain ratio and Gini, these may give subtly different results so it's worth having a go with a few of them.\n",
    "\n",
    "Ok, as with all machine learning, let's start by loading in some data. We'll use the Pima Indian Diabetes (public domain) data, obtained from Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "The data is stored as a CSV, which we can load in using the pandas library for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://github.com/ecs-vlc/ml_workshop_2019/raw/master/scikit-learn/pima-indians-diabetes-database.zip', 'pima-indians-diabetes-database.zip')\n",
    "\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('pima-indians-diabetes-database.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv('diabetes.csv', header=1, names=col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some data, we can peak at the top few rows using pandas `head` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the data into features and targets. Once we have these, we can use scikit-learn to create a train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Decision Tree Classifier\n",
    "\n",
    "Ok, we now have some data, that has been split so that we can think about training a model. We begin with a default DecisionTree from scikit-learn, and evaluate with a simple accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a trained model and an accuracy. It's a binary classification task (there are only two outcomes), so how do we know if this is good or not. Well, we can never be certain, but it helps to see what we would get if we just predicted that no one had diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage without diabetes:', 1 - (y.sum() / y.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our accuracy looks less impressive now, usually just a few percent above what you get by predicting the same thing each time. Our model could still have learned something useful though, maybe we should visualise it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising a Decision Tree Classifier\n",
    "\n",
    "First, we'll need to install some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz\n",
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, we can use a few lines of code to save an image of our tree and load it back in to the notebook (note, this probably won't work outside of Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can hopefully now see, the above tree looks significantly more complicated than we would like, especially if we are going to try to understand what it has learned. This leads us to the most common task in any machine learning, hyper-parameter optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Optimisation\n",
    "\n",
    "Hyper-parameters are any parameters that we control about our model. We have briefly discussed one already, the ASM. The sci-kit learn has a few others, the full list can be found in the docs: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "Here's an example of a better (accuracy) tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a much nicer accuracy for this problem. When the max depth of the tree is high, we run in to a common problem in machine learning, overfitting. Overfitting happens when you have a powerful model and not enough data, it just learns to reproduce the training data. That's why we always have a test set, it helps ensure we're not just overfitting. In the case of trees, limiting the depth is an immediate solution that helps the model to obtain better generlisation (generalising means that our model performs well on the test data and the training data).\n",
    "\n",
    "#### Interpretability\n",
    "\n",
    "Even though our model performs better, we'd still like something that we can interpret. Let's see how our new tree looks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('diabetes_smaller.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, it's a little better but still not great. Use the space below (or go back and re-run the cells) to try limiting the depth a bit more (perhaps 3) and visualise the tree again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Suppose now that accuracy is our primary concern, can we do any better? The random forest classifier uses a collection of decision trees (which each learn different patterns) to classify the data. Specifically, we train a whole forest (each tree on a different part of the data) and then, at test time, choose the best class.\n",
    "\n",
    "We can do this easily with sci-kit learn using the `RandomForestClassifier`: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create Decision Tree classifer object\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=1)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some hyper-parameter tuning we get something slightly better. The cost is interpretability, we can no longer draw our decision tree. However, we can at least print out the importance of each input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(zip(feature_cols, clf.feature_importances_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps unsurprisingly, the top three features are `glucose level`, `bmi` and `age`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data\n",
    "\n",
    "The techniques you have just used / learned can all be applied to any data which has this format (features with numeric values and a zero or one target). See if you can load in some other data and perform the classification task using a tree or random forest. A good example dataset would be the Heart Disease UCI data on kaggle (you may need an account to access it): https://www.kaggle.com/ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
